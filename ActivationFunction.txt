Activation Function
  1. 활성화 함수의 목적
    1) 입력층: x1, x2
       은닉층: h1, h2
       가중치: w1, w2, w3, w4
       출력층: y^
      (1) h1 = w1x1 + w2x2 + b1
      (2) h2 = w3x1 + w4x2 + b2 
      (3) y^ = v1h1 + v2h2 + b3
         :: 인공 신경망의 1차 결합 형태, 
            1차식에 1차식을 합성하여도 1차식이다. 
         ex) f(x) = ax + b, g(x) = cx + d
             g(f(x)) = c(ax+b)+d = acx + bc + d :: 결과도 1차 함수 
         :: 장점) 1차 함수의 용이함은 미분이 쉽다는 점이다. 
         :: 단점) 1차 함수의 특성으로 Linear한 그래프만 생성된다. 
     
     2) 인공신경망의 기본 연산 데이터가 1차식에 머무르기 때문에 선형적인 
        데이터 분류만 가능하다. 이러한 점은 XOR Gate와 같은 문제에서 
        한계에 부딪히게 된다. 
        활성화함수는 1차식의 선형그래프를 비선형화 만드는 함수이다. 
                    
  2. 여러 종류의 활성화 함수
    1) Sigmoid Function
       0.5 기준으로 0.5이하면 0, 초과면 1로 변형하는 활성화 함수 
       입력값에 대한 결과로 2가지를 추출하기 때문에 Binary Classification 문제에서 다룬다. 
       sigma = 1 / 1 + e의 -x승 의 식을 갖는다. 
       물론 다중분류에서도 사용이 가능하다. 
       
    2) Tanh Function
       0과 1사이의 값만을 취했던 Sigmoid와 다르게 Tanh는 -1 ~ 1값을 사용한다. 
       또한 0 부근에서 Sigmoid에 비하여 가파른 기울기를 갖는 특징이있다.
       a(x) = tanh(x)
       
    3) ReLU Function (Rectified Linear Unit)
       ReLU(x) = max(0,x)
       두개의 직선을 이어 붙여 만든 비선형 함수이다. 음수값부터 0까지의 직선과 0부터 양수값까지의 직선이 결합된 형태로
       비선형 구조이나 직선을 이어 붙여서 선형함수와 유사한 형태를 갖는다. 선형 함수의 경우 미분이 쉽다는 장점이 그대로 적용되며
       모델 최적화에 필요한 선형적 성질들을 보존할 수 있어서 최적화에 유리하다.
       미분에 대한 값이 0인 경우 미분을 할 수 없기 때문에 0인경우는 미분 값을 0으로 정의한다. 
       
    4) LeakyReLU Function
       ReLU를 약간 변형하여 만든 함수로 ReLU의 음수값이 기울기가 0인데 LeakyReLU는 음수값의 기울기를 작은 양수로 쥐어주어
       음수에 대한 연산도 가능하도록 개선되었다.
       
    5) ELU(Exponential Linear Unit)
       ReLU 음수의 그래프를 지수함수를 이용하여 X =< 0 인 부분을 Smooth 처리한 곡선 함수로 만들었다.
       
    6) SoftMax Function
       e의 x의 성분을 각각 k부터 n까지의 e의xk의 합으로 나눈다. 따라서 소프트 맥스의 모든 성분의 합은 항상 1이고 따라서 각 성분이
       확률 값을 갖게 된다. 여기서 e의 x를 몫으로 놓은 이유는 몫에 지수를 넣어 음수에 대한 성분이 나와 확률에 대한 성분으로 사용이 어려워진다.
       :: 지수 함수의 특성은 모든 x의 값에 대해서 y값은 항상 양수를 갖는다.
       Swish, SELU, maxout, Step Function등이 존재한다. 
       
       
       
       
  
     
