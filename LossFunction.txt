Loss Function
  1. 지도학습의 메커니즘
     - Input Data -> Model = f(w) -> Output y^ -> Loss Function L(y, y^) -> min L(y, y^) -> update w' -> Model = f(w') -> ....
       예측값 y^이 실제 y과 얼마 정도의 차이가 존재하는 가를 따져 모델 함수에 가중치를 업데이트 하는 방식으로 진행된다.
  
  2. 손실 함수 개요 (지도학습)  
     1) 회귀 문제 (Regression)
        - 회귀 문제는 우리가 원하는 결과값 즉, y값이 연속적인 변수임을 예측하는 문제이다.
          
     2) 분류 문제 (Claasification)
        - 분류 문제는 우리가 원하는 결과값이 유한한 변수임을 예측하는 문제이다.
          분류 문제는 2가지 방법으로 표현이 가능하다.
          (1) 라벨링: 숫자로 객체를 표현 
          (2) 원핫벡터: 0, 1만을 이용하여 객체를 1차원 벡터로 표현 
              ex) 객체   호랑이 /  개  / 고양이
                  라벨링   0       1       2
               원핫벡터 (0,0,1) (0,1,0) (1,0,0)
               
     3) 잔차(residual): 실제값과 예측값의 수직거리. 
               
     4) 손실 함수의 종류
        (1) 평균 절대 오차 (Mean Absolute Error, MAE)
            - 예측값 y^과 실제값 y의 수직 거리값(절대값)의 평균으로 표현된 오차
              절대값이므로 미분 불가 지역이 발생된다.
              
        (2) 평균 제곱 오차 (Mean Square Error, MSE)
            - 예측값 y^과 실제값 y의 수직 거리값의 제곱의 평균으로 표현된 오차
              제곱값이므로 모든 구간에서 미분이 가능하다.
              예측값과 실제값의 차이가 큰 경우에 제곱을 연산하기 때문에 Loss 값이 커져 
              성능이 MAE이에 비하여 안좋을 수 있다. 
       
        (3) 평균 제곱근 오차 (Root Mean Square Error, RMSE) 
            - 오차를 제곱한 MSE를 원래의 데이터 크기로 맞춰주기 위하여 root를 계산한다.
              단순 데이터 사이즈를 조절하기 위한 손실함수이다. 
              
      5) 분류 문제에서 대표적인 손실 함수의 종류
        (1) 내적 (Inner product)
            - 두개의 벡터가 a = (a1, a2, a3, .... , an), b = (b1, b2, b3, ... , bn)라고 하면 
              a x b = a1 x b1 + a2 x b2 + a3 x b3 + .... + an x bn 이다.
        
        (2) 교차 엔트로피 함수 (Cross Entropy Function)
            - 3개 이상의 다중 분류에서 사용된다. 실제값 y는 원핫벡터이고 log(softmax(예측값 y^))과 내적하는 함수이다.
              ex) 실제값 y(1, 0, 0)와 log(Softmax(예측값 y^)은 (a, b, c)일때 둘의 내적은 a + b x 0 + c x 0 = a가 나온다. 
                  
                
            
            
            
            
            
            
            
            
            
            
